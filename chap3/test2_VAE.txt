AE를 VAE로 바꾸는법
인코더와 손실 함수를 바꾼다

112p
AE가 잠재 공간의 한 포인트에 직접매핑
VAE는 잠재공간의 포인트 주변의 다변량 정규 분포에 매핑

※ 다변량 정규분포란?
https://blog.naver.com/mj93m/221097578389
여러 차원으로 확장한 정규분포, 평균은 벡터로, 분산은 행렬로 표현한다

일차원 정규 분포의 확률 밀도 함수는
f(x|μ, σ^2) = (2/(sqrt(2πσ^2)) * e^(-(x-μ)^2/2σ^2)

다변량 표준 정규 분포 N(0, I)는
평균 벡터가 영벡터이고, 공분산 행렬이 단위 벡터인 다변량 분포이다.

※ 공분산 구하는법
https://velog.io/@cuckoobee/%EA%B3%B5%EB%B6%84%EC%82%B0covariance-%EA%B3%B5%EB%B6%84%EC%82%B0-%ED%96%89%EB%A0%ACcovariance-matrix-%EC%83%81%EA%B4%80%EA%B3%84%EC%88%98Correlation-coefficient
https://blog.naver.com/PostView.naver?blogId=waterforall&logNo=222789143718

공분산은 각각의 변수들에서 기댓값(= 평균)을 뺀값들을 곱한것 ex) (X-E(X)) * (Y-E(Y)) ...
을 평균내서(= 기댓값을 구해서) 구해진다

평균벡터가 μ이고 등방성 공분산 행렬이 Σ인 k차원 다변량 정규 분포에 대한 확률 밀도 함수에 관한식은
113p

?!? 그렇다면

---

114p
책의 정규분포라는 단어는 일반적으로 등방향 다변량 분포라고 한다 N(0, I)
분산은 항상 양수이므로 실제로는 분산의 로그에 매핑한다고 한다??? -> 실제 사용되는 방식을 알아보자
분산 로그의 범위는 (-INF, INF) 모든 실수 가능

입력이미지를 인코더로 사용될 신경망에 넣어
(평균 벡터와 로그 분산 벡터로 다변량 표준 정규 분포(의 근사)에  매핑 가능)
-> (각각의 특징들의 평균의 벡터와 로그 분산 벡터에 신묘한 수학을 사용해 표준편차로 만든것에
노이즈(가우시안)를 곱한것을 더한것이 하나의 포인트 즉 하나의샘플링)

z_mean: 분포의 평균 벡터
z_log_var: 차원별 분산의 로그값

포인트 z의 샘플링 방식
z = z_mean + z_sigma * epsilon
z_sigma = exp(z_log_var * 0.5)
epsilon ~ N(0, I)

※ ~는 통계학에 앞의 값이 뒤의 확률 분포에서 나온다는 의미, 즉 엡실론은 다변량 표준 정규 분포에서 뽑아온다는 뜻
※ σ = exp(log(σ)) = exp(2log(σ)/2) = exp(log(σ^2)/2)
아까 로그를 취한 분산 즉, log_var는 표준편차의 제곱 즉 σ^2이고
이 로그값을 0.5배한것을 exp에 넣어주면 원래 표준편차와 크게 다를것이 없는 모양이다
??? 아마도 부호 문제때문?

https://angeloyeo.github.io/2020/10/11/VAE.html
첫 번째 한계점에 대한 보완책을 참고

Q: 그래서 이게 뭐하는 짓인가?
A: (인코더의 출력인) 잠재 공간을 연속적으로 만드는 것이다
Q: 그게 뭐가 좋은가?
A: z_mean 주변의 랜덤한 포인트를 샘플링하여 ->
같은 영역에 위치한 포인트라면 비슷한 이미지로 디코딩 된다 ->
본 적 없는 포인트에 대해 디코더가 제대로 디코딩

이 책에서 공분산 행렬은 대각행렬을 쓰는데, 이는
분포가 각 차원에서 독립적임을 의미한다
(= 벡터의 각 원소를 독립적인 평균과 분산을 가진 정규 분포에서 샘플링 가능)
VAE에서 다변량 정규 분포를 사용하는게 이런경우라했는데
모든 VAE가 '등방성'인지는 몰???루

여기의 '샘플링'이란 단어는 말그대로의 표본뜨기와,
데이터가 인코더를 통과한 특징 벡터에
노이즈를 주어 점을 찍는(=다변량 정규 분포에 매핑하는)거
모두를 의미하는듯하다???

117p
재매개변수화 트릭이란?
파라미터 z_mean과 z_log_var로 정의된 정규 분포에서 직접 샘플링하는 대신,
표준 정규 분포에서 epsilon을 샘플링한 다음 올바른 평균과 분산을 갖도록 샘플을 수동으로 조정
(평균에서 몇 시그마 떨어져있는지만 무작위 노이즈로 주고 평균과 (로그)분산은 학습을 시켜버리는듯)
오차역전파가능한 매개변수로 바꾸는 작업인듯하다

층의 모든 무작위성을 변수 epsilon에 포함 함으로써,
층의 입력에 대한 출력의 편도함수를
결정론적(= epsilon과 무관함)으로 표시 가능하다

※편도함수는 편미분한 함수 선택한 변수가 아니면 상수로 취급, https://m.blog.naver.com/falcon2026/221399596180)
※※ 결정론이란 사전적의미로는 과거의 원인이 미래의 결과가 되고 이 세상 모든 사건이 이미 정해진 곳에서 정해진 때에 이루어진다는 의미이며
'결정론적이다'의 의미를 머신러닝에서 추측해보자면 베이지안 통계학에서 데이터 관측후 사후확률이 정해지는것을 의미하는듯하다.
책에서 결정론적의 의미를 epsilon과 무관함으로 적어뒀는데, 무작위가 아닌 데이터 관측에 대한 인과에 따라 값이 조정되는것을 나타내는것같다.
https://untitledtblog.tistory.com/136
https://aws.amazon.com/ko/what-is/machine-learning/

????????
평균과 표준편차를 데이터셋에서 바로 계산 하는게 아니라 '학습'한다
확률 분포에 따라 샘플링한 데이터가 중간에 있기 때문에, 편미분, 역전파가 가능하도록 재매개변수화 트릭을 사용하는듯
https://bnmy6581.tistory.com/21

118p
!!!
평균과 표준편차를 학습하도록 한다
(5/1 추가, 평균과 표준편차를 학습하는 방법 자체가 쿨백 라이블러 발산인듯, 규제를 통해 어느 값으로 몰아넣음)

119p
원본 AE의 손실함수는 인코더-디코더를 통과한
출력과 원본이미지 사이의 재구성 손실로 구성된다
그러나 VAE는 추가로 쿨백-라이블러 발산을 사용한다


Q: KL발산이 뭔데
A: KL발산은 확률분포가 다른 확률 분포와 얼마나 다른지 측정하는 도구
Q: 어떻게 쓰는데
A: 손실함수에 KL 발산항을 추가함
Q: 쓰면 어떻게 되는데
A: 그 값을 최소화 하는 과정에서 z_mean과 z_log_var가 (0, 0)에 가까워진다
Q: 그게 뭘 의미하나?
A: 평균과 분산의 로그값이 0에 가까워진다는것은
곧 인코딩된 특징벡터의 분포가 표준 정규 분포를 가까워지는것을 의미한다
Q: 인코딩된 특징벡터가 표준 정규 분포를 따르면 뭐가 좋지?
A: 포인트 군집간 간격이 줄어들고, 이에 따라 인코더는 원점 주변의 공간을 효과적으로 사용하려고 학습한다
또한 잠재 공간에서 포인트를 선택할 때(= 생성할 때) 사용가능한, 잘 정의된 확률 분포를 가지게된다.
Q: KL 발산값의 식은 어떻게 구하는가
A: 평균 z_mean과 분산이 z_log_var인 정규 분포가 표준 정규 분포와 얼마나 다른지를 측정하는데
그 값을 계산하는 식은
kl_loss = -0.5 * sum(1 + z_log_var - z_mean^2 - exp(z_log_var))
수학적 표기는
D_KL[N(μ, σ)||N(0,1)] = -1/2 Σ (1 + log(σ^2)-μ^2-σ^2)

???! 왜 저런식이 나오는지, KL발산에 관해 자세히 알아보고 도출해보자
https://ko.wikipedia.org/wiki/%EC%BF%A8%EB%B0%B1-%EB%9D%BC%EC%9D%B4%EB%B8%94%EB%9F%AC_%EB%B0%9C%EC%82%B0
'의미'항이 의미심장하다

KL발산은 어디까지나 손실함수에 추가되는 규제이므로 하이퍼 파라미터를 통한 원래 손실값과의 비율이 필요하다

AE vs VAE 차이점 요약
VAE는 AE에서
인코더의 평균과 분산을 학습가능하도록 재매개변수화 트릭을 사용하고
손실함수에 KL 발산값을 더해 규제함으로써
데이터셋의 잠재공간 인코딩할 때 다변량 표준 정규 분포에 근사하도록한다.

self-숙제 ???!
VAE를 파이토치로(가능하다면 fashion MNIST까지) 포팅된 코드 찾아보
VAE가 stable diffusion의 어디에서 사용되는지 확인해보기
encoder&decoder를 더 좋은 합성곱으로 대체가능한지, decode는 diffusion으로만 쓰는지 활용

122p부터 하면됩니다

재구축 오차(오토인코더의 출력과 원본이미지 사이의 오차)와
쿨백 라이블러 발산을 동시에 사용하는 직관적인 그림
https://www.jeremyjordan.me/variational-autoencoders/

학습 끝난 VAE에서 무작위 생성하는법
1. (가우시안) 노이즈를 특징벡터의 차원만큼 만듬
2. 그걸 그대로 디코더에 넣고 predict
???
어캐 그게 가능하노?
'애시당초에' 학습할 때 데이터셋이 잠재공간에 매핑되는거 자체가 다변량 표준 정규 분포라서,
거기에서 값을 꺼내오는것도 가능
위의 문제에서 '값을 가져올 분포가 존재하지 않는' 문제가 해결된셈
test1의 109p와 110p를 잘보자

130p
vae_utils < 이거 뭐하는 코드인지부터 까봅시다
개념적으로는
라벨링된 벡터들의 평균에서
이미지 전체의 평균벡터를 빼면
라벨에 해당하는 (라벨의 특징으로 향하는) 벡터를 구할 수가 있다고 한다

utils의 코드는 get_vector_from_label이 핵심인데
이 코드의 핵심은 이미지를 둘로 나누고 벡터를 구하는것이다
벡터의 합, 벡터의 수를 가지고 벡터의 평균 2개(속성이 있는것, 없는것)을 가지고
이 둘을 빼서 속성으로하 향하는 벡터(방향성)을 구하는것이다
코드 상에는 갱신된 벡터가 기존과 적어지면 중단하는 것과
찾아낸 속성 특징벡터를 단위 벡터화하여 반환하는것도 포함되어있다